{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ignore this CNN-XGBoost code and go below for pure XGboost code"
      ],
      "metadata": {
        "id": "62RwgHhqhxVD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "J89TPrvbKK0H",
        "outputId": "c323c87e-41ba-45a7-d1f3-fbb3aaaf8817"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3abe6f8b6230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'BatchNormalization' from 'keras.layers.normalization' (/usr/local/lib/python3.7/dist-packages/keras/layers/normalization/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# https://youtu.be/2miw-69Xb0g\n",
        "\n",
        "\"\"\"\n",
        "@author: Sreenivas Bhattiprolu\n",
        "IMAGE CLASSIFICATION USING XGBOOST by extracting features using VGG16 imagenet pretrained weights.\n",
        "This code explains the process of using XGBoost for image classification\n",
        "using pretrained weights (VGG16) as feature extractors.\n",
        "Code last tested on: \n",
        "    Tensorflow: 2.2.0\n",
        "    Keras: 2.3.1\n",
        "    Python: 3.7\n",
        "pip install xgboost  \n",
        "    \n",
        "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
        "              importance_type='gain', interaction_constraints='',\n",
        "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
        "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
        "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
        "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
        "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
        "              tree_method='exact', validate_parameters=1, verbosity=None)   \n",
        "    \n",
        "\"\"\"\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import os\n",
        "import seaborn as sns\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "\n",
        "# Read input images and assign labels based on folder names\n",
        "print(os.listdir(\"images/classification/\"))\n",
        "\n",
        "SIZE = 256  #Resize images\n",
        "\n",
        "#Capture training data and labels into respective lists\n",
        "train_images = []\n",
        "train_labels = [] \n",
        "\n",
        "for directory_path in glob.glob(\"images/classification/train/*\"):\n",
        "    label = directory_path.split(\"\\\\\")[-1]\n",
        "    print(label)\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
        "        print(img_path)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
        "        img = cv2.resize(img, (SIZE, SIZE))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        train_images.append(img)\n",
        "        train_labels.append(label)\n",
        "\n",
        "#Convert lists to arrays        \n",
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "\n",
        "# Capture test/validation data and labels into respective lists\n",
        "\n",
        "test_images = []\n",
        "test_labels = [] \n",
        "for directory_path in glob.glob(\"images/classification/validation/*\"):\n",
        "    fruit_label = directory_path.split(\"\\\\\")[-1]\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        img = cv2.resize(img, (SIZE, SIZE))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        test_images.append(img)\n",
        "        test_labels.append(fruit_label)\n",
        "\n",
        "#Convert lists to arrays                \n",
        "test_images = np.array(test_images)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "#Encode labels from text to integers.\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(test_labels)\n",
        "test_labels_encoded = le.transform(test_labels)\n",
        "le.fit(train_labels)\n",
        "train_labels_encoded = le.transform(train_labels)\n",
        "\n",
        "#Split data into test and train datasets (already split but assigning to meaningful convention)\n",
        "x_train, y_train, x_test, y_test = train_images, train_labels_encoded, test_images, test_labels_encoded\n",
        "\n",
        "###################################################################\n",
        "# Normalize pixel values to between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "#One hot encode y values for neural network. \n",
        "# from keras.utils import to_categorical\n",
        "# y_train_one_hot = to_categorical(y_train)\n",
        "# y_test_one_hot = to_categorical(y_test)\n",
        "\n",
        "#############################\n",
        "#Load model wothout classifier/fully connected layers\n",
        "VGG_model = VGG16(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))\n",
        "\n",
        "#Make loaded layers as non-trainable. This is important as we want to work with pre-trained weights\n",
        "for layer in VGG_model.layers:\n",
        "\tlayer.trainable = False\n",
        "    \n",
        "VGG_model.summary()  #Trainable parameters will be 0\n",
        "\n",
        "\n",
        "#Now, let us use features from convolutional network for RF\n",
        "feature_extractor=VGG_model.predict(x_train)\n",
        "\n",
        "features = feature_extractor.reshape(feature_extractor.shape[0], -1)\n",
        "\n",
        "X_for_training = features #This is our X input to RF\n",
        "\n",
        "#RANDOM FOREST\n",
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "#model = RandomForestClassifier(n_estimators = 50, random_state = 42)\n",
        "\n",
        "# Train the model on training data\n",
        "\n",
        "\n",
        "#XGBOOST\n",
        "import xgboost as xgb\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X_for_training, y_train) #For sklearn no one hot encoding\n",
        "\n",
        "#Send test data through same feature extractor process\n",
        "X_test_feature = VGG_model.predict(x_test)\n",
        "X_test_features = X_test_feature.reshape(X_test_feature.shape[0], -1)\n",
        "\n",
        "#Now predict using the trained RF model. \n",
        "prediction = model.predict(X_test_features)\n",
        "#Inverse le transform to get original label back. \n",
        "prediction = le.inverse_transform(prediction)\n",
        "\n",
        "#Print overall accuracy\n",
        "from sklearn import metrics\n",
        "print (\"Accuracy = \", metrics.accuracy_score(test_labels, prediction))\n",
        "\n",
        "#Confusion Matrix - verify accuracy of each class\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(test_labels, prediction)\n",
        "#print(cm)\n",
        "sns.heatmap(cm, annot=True)\n",
        "\n",
        "#Check results on a few select images\n",
        "n=np.random.randint(0, x_test.shape[0])\n",
        "img = x_test[n]\n",
        "plt.imshow(img)\n",
        "input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
        "input_img_feature=VGG_model.predict(input_img)\n",
        "input_img_features=input_img_feature.reshape(input_img_feature.shape[0], -1)\n",
        "prediction = model.predict(input_img_features)[0] \n",
        "prediction = le.inverse_transform([prediction])  #Reverse the label encoder to original name\n",
        "print(\"The prediction for this image is: \", prediction)\n",
        "print(\"The actual label for this image is: \", test_labels[n])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from xgboost import XGBClassifier\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "OgSp_EQ2eTGI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "# train = tfds.load('food101', split='train', shuffle_files=True)"
      ],
      "metadata": {
        "id": "_8oXiE4Nflq8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = tfds.load('food101', split='validation', shuffle_files=True)"
      ],
      "metadata": {
        "id": "jMAFJfSkibkN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# info = tfds.builder('food101').info\n",
        "# train_new = tfds.as_dataframe(train,info)\n",
        "test_new = tfds.as_dataframe(test)"
      ],
      "metadata": {
        "id": "9h_20LIWiHOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_new"
      ],
      "metadata": {
        "id": "9PP_-GWVinzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(featuresdf.feature.tolist())\n",
        "y = np.array(featuresdf.class_label.tolist())\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "JOb7g0QIemoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simpler model\n",
        "# define the model\n",
        "model = XGBClassifier(max_depth = 6, objective = 'multi:softmax', num_class = 10)\n",
        "# define the datasets to evaluate each iteration\n",
        "evalset = [(x_train, y_train), (x_test,y_test)]\n",
        "# fit the model\n",
        "model.fit(x_train, y_train, eval_set = evalset)"
      ],
      "metadata": {
        "id": "brt6pz6phkHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameter tuner+model version\n",
        "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
        "space={'max_depth': hp.quniform(\"max_depth\", 2, 10, 1),\n",
        "        'gamma': hp.uniform ('gamma', 0,1),\n",
        "        'reg_alpha' : hp.quniform('reg_alpha', 0,20,1),\n",
        "        'min_child_weight' : hp.quniform('min_child_weight', 0, 6, 1),\n",
        "       'learning_rate': hp.uniform('learning_rate',1e-2, 1e-1),\n",
        "       'max_delta_step':hp.quniform('max_delta_step',0,20,1),\n",
        "    }\n",
        "def objective(space):\n",
        "    clf=XGBClassifier(objective = 'multi:softmax', num_class = 10,\n",
        "                    max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
        "                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n",
        "                     learning_rate=space['learning_rate'], max_delta_step=space['max_delta_step'],seed=0)\n",
        "    \n",
        "    evaluation = [(x_train, y_train), (x_test, y_test)]\n",
        "    \n",
        "    clf.fit(x_train, y_train,\n",
        "            eval_set=evaluation, eval_metric=\"auc\",\n",
        "            early_stopping_rounds=10,verbose=False)\n",
        "    \n",
        "\n",
        "    pred = clf.predict(x_test)\n",
        "    accuracy = accuracy_score(y_test, pred>0.5)\n",
        "    print (\"SCORE:\", accuracy)\n",
        "    return {'loss': -accuracy, 'status': STATUS_OK }"
      ],
      "metadata": {
        "id": "oi8rUpVAeftN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trials = Trials()\n",
        "\n",
        "best_hyperparams = fmin(fn = objective,\n",
        "                        space = space,\n",
        "                        algo = tpe.suggest,\n",
        "                        max_evals = 100,\n",
        "                        trials = trials)"
      ],
      "metadata": {
        "id": "15a3-UfLhJkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best hyperparameters are : \",\"\\n\")\n",
        "print(best_hyperparams)\n",
        "# define the model\n",
        "model_new = XGBClassifier(learning_rate=0.09916621749041565,objective = 'multi:softmax', num_class = 10,\n",
        "                    max_depth = 9, gamma = 0.5340419606937814,max_delta_step=16,\n",
        "                    reg_alpha = 2, min_child_weight=0, seed=0, n_estimators = 200)\n",
        "#good hyperparam: learn=0.1, max depth=11, gamma=0,reg=0,min child=0\n",
        "#learning rate affects gradient descent, does not impact accuracy\n",
        "#reg_alpha, gamma reduces the divergence between train/validation curves\n",
        "#max depth and min_child_weight improves accuracy\n",
        "# define the datasets to evaluate each iteration\n",
        "evalset = [(x_train, y_train), (x_test,y_test)]\n",
        "# fit the model\n",
        "model_new.fit(x_train, y_train, eval_set = evalset, early_stopping_rounds=10)"
      ],
      "metadata": {
        "id": "xhBBwVJehLh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate performance\n",
        "pred_train = model_new.predict(x_train)\n",
        "trainscore = accuracy_score(y_train, pred_train)\n",
        "print('Train Accuracy:', trainscore)\n",
        "pred = model_new.predict(x_test)\n",
        "score = accuracy_score(y_test, pred)\n",
        "print('Validation Accuracy:', score)\n",
        "# retrieve performance metrics\n",
        "# results = model.evals_result()\n",
        "# # plot learning curves\n",
        "results = model_new.evals_result()\n",
        "pyplot.plot(results['validation_0']['mlogloss'], label='train')\n",
        "pyplot.plot(results['validation_1']['mlogloss'], label='test')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "W9ZL_aHFhU-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation\n",
        "eval_list = evalfeaturesdfnew['feature'].to_numpy().tolist()\n",
        "eval_pred = model.predict(eval_list)"
      ],
      "metadata": {
        "id": "UCgZNcf0hawo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}